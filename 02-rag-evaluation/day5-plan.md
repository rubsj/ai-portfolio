# P2 Day 5 Implementation Plan ‚Äî Precise Instructions for Sonnet

## Context Summary

**Project:** P2 RAG Evaluation Benchmarking Framework
**Day:** 5 of 5 (Final day)
**Time budget:** 4 hours
**Goal:** Create polished portfolio deliverables (visualizations, Streamlit demo, documentation)

**Current state (all complete from Days 1-4):**
- ‚úÖ 16 configs evaluated (grid search complete)
- ‚úÖ 4 charts exist: `config_heatmap.png`, `metric_comparison.png`, `bm25_comparison.png`, `semantic_vs_fixed.png`
- ‚úÖ All 5 ADRs written in `docs/adr/`
- ‚úÖ Results data ready in `results/metrics/`:
  - `grid_search_results.json` (38,615 lines, 16 ConfigEvaluation objects)
  - `reranking_results.json` (3 RerankingComparison objects)
  - `ragas_results.json` (1 RAGASResult for E-openai)
  - `judge_results.json` (56 JudgeResult objects)
  - `qa_dataset_report.json` (QA quality metrics)

**Top result:** E-openai (semantic chunking) achieved R@5: 0.625, improved to 0.747 with reranking (+19.5%)

---

## Task Execution Order (Priority-Based)

Execute in this exact order:

0. **MANDATORY FIRST:** Inspect actual JSON schemas and extract real values
1. **Task 27:** Generate 8 additional charts (~2 hours)
2. **Task 29:** Write README.md (~45 minutes)
3. **Task 26:** Build Streamlit app (~1 hour)
4. **Task 28:** Compile GridSearchReport (~15 minutes)
5. **Post-implementation:** Update CLAUDE.md, git commit/push/PR, Notion journal
6. **Task 25:** CLI (skip if time constrained)
7. **Task 30:** ADRs (skip ‚Äî already complete)

Stop after each task and wait for user approval before proceeding.

---

## STEP 0: Inspect Actual JSON Schemas (MANDATORY)

**WHY:** The plan initially hardcoded field names without verifying they match actual data. This step validates all field names and extracts real values for README.

**Run these commands first:**

```bash
cd /Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation

# Grid search results structure
python -c "import json; d=json.loads(open('results/metrics/grid_search_results.json').read()); print('Type:', type(d), 'Length:', len(d)); print('\n=== First config structure ==='); print(json.dumps(d[0], indent=2))" | head -80

# Reranking results structure
python -c "import json; d=json.loads(open('results/metrics/reranking_results.json').read()); print('Type:', type(d), 'Length:', len(d)); print('\n=== First reranking result ==='); print(json.dumps(d[0] if isinstance(d,list) else d, indent=2))" | head -80

# Extract all config R@5 values for README
python -c "
import json
data = json.loads(open('results/metrics/grid_search_results.json').read())
configs = {}
for item in data:
    key = f\"{item['chunk_config']}-{item['embedding_model']}\"
    configs[key] = {
        'r5': item['avg_recall_at_5'],
        'p5': item['avg_precision_at_5'],
        'mrr5': item['avg_mrr_at_5']
    }
print('=== All Config R@5 Values ===')
for config in ['A', 'B', 'C', 'D', 'E']:
    for model in ['minilm', 'mpnet', 'openai']:
        key = f'{config}-{model}'
        if key in configs:
            print(f'{key}: R@5={configs[key][\"r5\"]:.3f}, P@5={configs[key][\"p5\"]:.3f}')
bm25 = [x for x in data if x['retrieval_method'] == 'bm25'][0]
print(f\"BM25: R@5={bm25['avg_recall_at_5']:.3f}\")
"
```

**CONFIRMED VALUES (from actual data):**
- A-openai: R@5=0.304, P@5=0.100
- B-openai: R@5=0.607, P@5=0.229
- C-openai: R@5=0.512, P@5=0.189
- D-openai: R@5=0.529, P@5=0.225
- E-openai: R@5=0.625, P@5=0.232
- B-minilm: R@5=0.481, P@5=0.189
- B-mpnet: R@5=0.467, P@5=0.168
- BM25: R@5=0.381, P@5=0.129
- Reranking fields: `recall_at_5_before`, `recall_at_5_after`, `recall_improvement_pct` (FLAT, not nested!)
- RAGAS values: faithfulness=0.511, answer_relevancy=0.5625, context_recall=0.7125, context_precision=0.734
- Judge fields: `correctness_score` (bool), `has_hallucination` (bool), `bloom_level` (str), `relevance_grade` (str) ‚úì

**Proceed only after confirming these match your data.**

---

## TASK 27: Generate 8 Additional Charts

**File to modify:** `/Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation/src/visualization.py`

**Current state:** File has 335 lines with 4 chart functions already implemented.

### Step 1: Add 4 New Data Loader Functions

**Location:** After line 79 (after `load_evaluations()`), add these 4 functions:

```python
def load_reranking_results(path: Path | None = None) -> list[dict]:
    """Load reranking comparison results from JSON.

    WHY separate loader: reranking data has different structure from evaluations.
    Returns raw dicts because RerankingComparison model is already validated.
    """
    if path is None:
        path = METRICS_DIR / "reranking_results.json"

    data = json.loads(path.read_text())
    return data  # List of 3 dicts: E-openai, B-openai, D-openai


def load_ragas_results(path: Path | None = None) -> dict:
    """Load RAGAS evaluation results from JSON.

    WHY single dict: only one config (E-openai) was evaluated with RAGAS.
    """
    if path is None:
        path = METRICS_DIR / "ragas_results.json"

    data = json.loads(path.read_text())
    return data  # Single dict with faithfulness, answer_relevancy, etc.


def load_judge_results(path: Path | None = None) -> list[dict]:
    """Load LLM-as-Judge results from JSON.

    WHY list: 56 JudgeResult objects (one per question).
    """
    if path is None:
        path = METRICS_DIR / "judge_results.json"

    data = json.loads(path.read_text())
    return data  # List of 56 dicts


def load_qa_report(path: Path | None = None) -> dict:
    """Load QA dataset quality report from JSON.

    WHY separate: QADatasetReport is a summary, not per-question data.
    """
    if path is None:
        path = REPORTS_DIR / "qa_dataset_report.json"

    data = json.loads(path.read_text())
    return data  # Single dict with coverage, type distribution, etc.
```

**Import additions needed:** Add `REPORTS_DIR` to the import from config:
```python
from src.config import CHARTS_DIR, METRICS_DIR, REPORTS_DIR  # Add REPORTS_DIR
```

### Step 2: Add Chart 5 ‚Äî Chunk Size Effect

**Location:** After existing chart functions (around line 250), add:

```python
def generate_chunk_size_effect_chart(
    evaluations: list[ConfigEvaluation],
    output_path: Path | None = None,
) -> None:
    """Generate line chart showing chunk size impact on Recall@5.

    WHY line chart: shows trend as chunk size increases (128 ‚Üí 256 ‚Üí 512).
    Compares Configs A/B/C with constant 25% overlap and openai embeddings.

    Args:
        evaluations: All 16 configs
        output_path: Where to save PNG (defaults to CHARTS_DIR/chunk_size_effect.png)
    """
    if output_path is None:
        output_path = CHARTS_DIR / "chunk_size_effect.png"

    # Filter to Configs A, B, C with openai embeddings (constant overlap 25%)
    configs_of_interest = ["A-openai", "B-openai", "C-openai"]
    filtered = [ev for ev in evaluations if ev.config_id in configs_of_interest]

    # Map config to chunk size
    chunk_sizes = {"A-openai": 128, "B-openai": 256, "C-openai": 512}

    # Extract data
    data = []
    for ev in filtered:
        data.append({
            "chunk_size": chunk_sizes[ev.config_id],
            "recall_at_5": ev.avg_recall_at_5,
        })

    # Sort by chunk size
    data.sort(key=lambda x: x["chunk_size"])

    df = pd.DataFrame(data)

    # Create figure
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.plot(
        df["chunk_size"],
        df["recall_at_5"],
        marker="o",
        linewidth=2.5,
        markersize=10,
        color="steelblue",
        label="Recall@5",
    )

    ax.set_xlabel("Chunk Size (tokens)", fontsize=12, fontweight="bold")
    ax.set_ylabel("Recall@5", fontsize=12, fontweight="bold")
    ax.set_title("Chunk Size Impact on Retrieval Performance", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Annotate each point with value
    for _, row in df.iterrows():
        ax.annotate(
            f"{row['recall_at_5']:.3f}",
            xy=(row["chunk_size"], row["recall_at_5"]),
            xytext=(0, 10),
            textcoords="offset points",
            ha="center",
            fontsize=10,
        )

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Chunk size effect chart saved to {output_path}")
```

### Step 3: Add Chart 6 ‚Äî Overlap Effect

**Location:** After Chart 5, add:

```python
def generate_overlap_effect_chart(
    evaluations: list[ConfigEvaluation],
    output_path: Path | None = None,
) -> None:
    """Generate multi-panel comparison of Config B (25%) vs Config D (50%) overlap.

    WHY multi-panel: shows overlap impact across 3 key metrics (R@5, P@5, MRR@5).
    Uses openai embeddings to isolate the overlap variable.

    Args:
        evaluations: All 16 configs
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "overlap_effect.png"

    # Filter to B-openai (25%) and D-openai (50%)
    b_config = next(ev for ev in evaluations if ev.config_id == "B-openai")
    d_config = next(ev for ev in evaluations if ev.config_id == "D-openai")

    metrics = ["avg_recall_at_5", "avg_precision_at_5", "avg_mrr_at_5"]
    metric_labels = ["Recall@5", "Precision@5", "MRR@5"]

    # Create 1x3 subplots
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for ax, metric, label in zip(axes, metrics, metric_labels):
        b_value = getattr(b_config, metric)
        d_value = getattr(d_config, metric)

        x = ["Config B\n(25% overlap)", "Config D\n(50% overlap)"]
        y = [b_value, d_value]
        colors = ["steelblue", "coral"]

        bars = ax.bar(x, y, color=colors, alpha=0.8, edgecolor="black", linewidth=1.5)

        # Annotate bars with values
        for bar, value in zip(bars, y):
            height = bar.get_height()
            ax.annotate(
                f"{value:.3f}",
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 5),
                textcoords="offset points",
                ha="center",
                fontsize=11,
                fontweight="bold",
            )

        ax.set_ylabel(label, fontsize=12, fontweight="bold")
        ax.set_ylim(0, max(y) * 1.15)
        ax.grid(axis="y", alpha=0.3)

    fig.suptitle("Overlap Impact: 25% vs 50% (Configs B vs D, OpenAI Embeddings)", fontsize=14, fontweight="bold")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Overlap effect chart saved to {output_path}")
```

### Step 4: Add Chart 7 ‚Äî Embedding Model Comparison

**Location:** After Chart 6, add:

```python
def generate_embedding_comparison_chart(
    evaluations: list[ConfigEvaluation],
    output_path: Path | None = None,
) -> None:
    """Generate grouped bar chart comparing embedding models (minilm, mpnet, openai).

    WHY grouped bars: shows 3 metrics side-by-side for each embedding model.
    Uses Config B (256/64) to isolate embedding quality from chunk size.

    Args:
        evaluations: All 16 configs
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "embedding_comparison.png"

    # Filter to B-minilm, B-mpnet, B-openai
    configs_of_interest = ["B-minilm", "B-mpnet", "B-openai"]
    filtered = [ev for ev in evaluations if ev.config_id in configs_of_interest]

    # Sort by a logical order: minilm, mpnet, openai (quality ascending)
    order = {"B-minilm": 0, "B-mpnet": 1, "B-openai": 2}
    filtered.sort(key=lambda ev: order[ev.config_id])

    # Extract data
    model_labels = ["MiniLM\n(384d)", "MPnet\n(768d)", "OpenAI\n(1536d)"]
    recall_5 = [ev.avg_recall_at_5 for ev in filtered]
    precision_5 = [ev.avg_precision_at_5 for ev in filtered]
    mrr_5 = [ev.avg_mrr_at_5 for ev in filtered]

    x = np.arange(len(model_labels))
    width = 0.25

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.bar(x - width, recall_5, width, label="Recall@5", color="steelblue", alpha=0.8)
    ax.bar(x, precision_5, width, label="Precision@5", color="coral", alpha=0.8)
    ax.bar(x + width, mrr_5, width, label="MRR@5", color="seagreen", alpha=0.8)

    ax.set_xlabel("Embedding Model", fontsize=12, fontweight="bold")
    ax.set_ylabel("Score", fontsize=12, fontweight="bold")
    ax.set_title("Embedding Model Comparison (Config B: 256 tokens, 25% overlap)", fontsize=14, fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(model_labels)
    ax.legend()
    ax.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Embedding comparison chart saved to {output_path}")
```

### Step 5: Add Chart 8 ‚Äî Question Type Breakdown

**Location:** After Chart 7, add:

```python
def generate_question_type_breakdown_chart(
    evaluations: list[ConfigEvaluation],
    output_path: Path | None = None,
) -> None:
    """Generate stacked bar chart showing Recall@5 by question type.

    WHY stacked bars: shows how each config performs across different question types.
    Focuses on top-5 configs for readability.

    Args:
        evaluations: All 16 configs
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "question_type_breakdown.png"

    # Sort by avg_recall_at_5 and take top-5
    sorted_evals = sorted(evaluations, key=lambda ev: ev.avg_recall_at_5, reverse=True)
    top_5 = sorted_evals[:5]

    # Guard: check if metrics_by_question_type exists for at least one config
    # WHY: This field may be missing if evaluation was run without per-type breakdowns
    has_question_type_data = any(
        hasattr(ev, 'metrics_by_question_type') and ev.metrics_by_question_type
        for ev in top_5
    )

    if not has_question_type_data:
        logger.warning("‚ö†Ô∏è  Skipping question type breakdown chart ‚Äî metrics_by_question_type field not found in data")
        return

    # Extract question type metrics
    # Structure: metrics_by_question_type = {"factual": {"avg_recall_at_5": 0.5, ...}, ...}
    question_types = ["factual", "analytical", "multi_hop"]  # Most common types

    data = {qt: [] for qt in question_types}
    config_labels = []

    for ev in top_5:
        config_labels.append(ev.config_id)
        # Safe access with empty dict fallback
        type_metrics = getattr(ev, 'metrics_by_question_type', {}) or {}
        for qt in question_types:
            if qt in type_metrics:
                data[qt].append(type_metrics[qt]["avg_recall_at_5"])
            else:
                data[qt].append(0.0)  # No questions of this type

    x = np.arange(len(config_labels))
    width = 0.6

    fig, ax = plt.subplots(figsize=(12, 6))

    # Stacked bars
    bottom = np.zeros(len(config_labels))
    colors = ["steelblue", "coral", "seagreen"]

    for qt, color in zip(question_types, colors):
        ax.bar(x, data[qt], width, label=qt.replace("_", " ").title(), bottom=bottom, color=color, alpha=0.8)
        bottom += np.array(data[qt])

    ax.set_xlabel("Configuration", fontsize=12, fontweight="bold")
    ax.set_ylabel("Recall@5 (Stacked by Question Type)", fontsize=12, fontweight="bold")
    ax.set_title("Recall@5 Breakdown by Question Type (Top-5 Configs)", fontsize=14, fontweight="bold")
    ax.set_xticks(x)
    ax.set_xticklabels(config_labels, rotation=15, ha="right")
    ax.legend(title="Question Type")
    ax.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Question type breakdown chart saved to {output_path}")
```

### Step 6: Add Chart 9 ‚Äî Reranking Impact

**Location:** After Chart 8, add:

```python
def generate_reranking_impact_chart(
    output_path: Path | None = None,
) -> None:
    """Generate paired horizontal bar chart showing reranking before/after.

    WHY horizontal bars: easier to read config names and improvement percentages.
    Shows 3 configs that were reranked: E-openai, B-openai, D-openai.

    Args:
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "reranking_impact.png"

    # Load reranking results
    rerank_data = load_reranking_results()

    # Extract data for plotting
    # WHY flat field names: reranking JSON uses flat structure, not nested original_metrics/reranked_metrics
    config_labels = []
    before_scores = []
    after_scores = []
    improvements = []

    for item in rerank_data:
        config_labels.append(item["config_id"])
        before_scores.append(item["recall_at_5_before"])  # FIXED: was "before_recall_at_5"
        after_scores.append(item["recall_at_5_after"])    # FIXED: was "after_recall_at_5"
        improvements.append(item["recall_improvement_pct"])  # FIXED: was "improvement_percent_recall_at_5"

    y = np.arange(len(config_labels))
    height = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))

    # Horizontal bars
    bars1 = ax.barh(y - height/2, before_scores, height, label="Before Reranking", color="lightcoral", alpha=0.8)
    bars2 = ax.barh(y + height/2, after_scores, height, label="After Reranking", color="seagreen", alpha=0.8)

    ax.set_yticks(y)
    ax.set_yticklabels(config_labels)
    ax.set_xlabel("Recall@5", fontsize=12, fontweight="bold")
    ax.set_title("Reranking Impact on Top-3 Configurations", fontsize=14, fontweight="bold")
    ax.legend()
    ax.grid(axis="x", alpha=0.3)

    # Annotate improvement percentages
    for i, (before, after, improvement) in enumerate(zip(before_scores, after_scores, improvements)):
        ax.annotate(
            f"+{improvement:.1f}%",
            xy=(max(before, after) + 0.02, y[i]),
            fontsize=11,
            fontweight="bold",
            color="darkgreen",
        )

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Reranking impact chart saved to {output_path}")
```

### Step 7: Add Chart 10 ‚Äî RAGAS Radar Chart

**Location:** After Chart 9, add:

```python
def generate_ragas_radar_chart(
    output_path: Path | None = None,
) -> None:
    """Generate radar/polar chart showing 4 RAGAS metrics for best config.

    WHY radar chart: shows performance across multiple dimensions on same scale.
    All RAGAS metrics are 0-1, making them directly comparable.

    Args:
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "ragas_radar.png"

    # Load RAGAS results
    ragas_data = load_ragas_results()

    # Extract metrics (all 0-1 scale)
    metrics = [
        ("Faithfulness", ragas_data["faithfulness"]),
        ("Answer Relevancy", ragas_data["answer_relevancy"]),
        ("Context Recall", ragas_data["context_recall"]),
        ("Context Precision", ragas_data["context_precision"]),
    ]

    labels = [m[0] for m in metrics]
    values = [m[1] for m in metrics]

    # Number of variables
    num_vars = len(labels)

    # Compute angle for each axis
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()

    # Complete the circle
    values += values[:1]
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection="polar"))

    ax.plot(angles, values, "o-", linewidth=2, color="steelblue", label="E-openai")
    ax.fill(angles, values, alpha=0.25, color="steelblue")

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=11)
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(["0.2", "0.4", "0.6", "0.8", "1.0"], fontsize=9)
    ax.grid(True, alpha=0.3)

    ax.set_title("RAGAS Metrics ‚Äî Best Configuration (E-openai)", fontsize=14, fontweight="bold", pad=20)
    ax.legend(loc="upper right", bbox_to_anchor=(1.2, 1.1))

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì RAGAS radar chart saved to {output_path}")
```

### Step 8: Add Chart 11 ‚Äî Bloom Distribution

**Location:** After Chart 10, add:

```python
def generate_bloom_distribution_chart(
    output_path: Path | None = None,
) -> None:
    """Generate horizontal bar chart showing Bloom taxonomy distribution.

    WHY horizontal bars: easier to read Bloom level names (Remember, Understand, etc.).
    Shows cognitive complexity distribution of the 56 questions.

    Args:
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "bloom_distribution.png"

    # Load judge results
    judge_data = load_judge_results()

    # Count questions per Bloom level
    bloom_counts = {}
    for item in judge_data:
        bloom_level = item.get("bloom_level")
        if bloom_level and bloom_level != "None":  # Filter out None values
            bloom_counts[bloom_level] = bloom_counts.get(bloom_level, 0) + 1

    # Sort by Bloom taxonomy order
    bloom_order = ["Remember", "Understand", "Apply", "Analyze", "Evaluate", "Create"]
    sorted_items = [(level, bloom_counts.get(level, 0)) for level in bloom_order if level in bloom_counts]

    labels = [item[0] for item in sorted_items]
    counts = [item[1] for item in sorted_items]

    fig, ax = plt.subplots(figsize=(10, 6))

    bars = ax.barh(labels, counts, color="steelblue", alpha=0.8, edgecolor="black", linewidth=1.5)

    # Annotate bars with counts
    for bar, count in zip(bars, counts):
        width = bar.get_width()
        ax.annotate(
            f"{count}",
            xy=(width, bar.get_y() + bar.get_height() / 2),
            xytext=(5, 0),
            textcoords="offset points",
            ha="left",
            va="center",
            fontsize=11,
            fontweight="bold",
        )

    ax.set_xlabel("Number of Questions", fontsize=12, fontweight="bold")
    ax.set_ylabel("Bloom Taxonomy Level", fontsize=12, fontweight="bold")
    ax.set_title("Question Distribution by Cognitive Complexity (Bloom Taxonomy)", fontsize=14, fontweight="bold")
    ax.grid(axis="x", alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì Bloom distribution chart saved to {output_path}")
```

### Step 9: Add Chart 12 ‚Äî QA Dataset Quality Dashboard

**Location:** After Chart 11, add:

```python
def generate_qa_quality_dashboard(
    output_path: Path | None = None,
) -> None:
    """Generate 2x2 subplot grid showing QA dataset quality metrics.

    WHY 2x2 grid: shows 4 different quality dimensions in one comprehensive view.
    Includes type distribution, hierarchy, strategy, and coverage.

    Args:
        output_path: Where to save PNG
    """
    if output_path is None:
        output_path = CHARTS_DIR / "qa_quality.png"

    # Load QA quality report
    qa_data = load_qa_report()

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # Subplot 1: Question Type Distribution (top-left)
    ax1 = axes[0, 0]
    type_dist = qa_data["questions_per_type"]
    ax1.bar(type_dist.keys(), type_dist.values(), color="steelblue", alpha=0.8)
    ax1.set_title("Question Type Distribution", fontsize=12, fontweight="bold")
    ax1.set_ylabel("Count")
    ax1.tick_params(axis="x", rotation=45)
    ax1.grid(axis="y", alpha=0.3)

    # Subplot 2: Hierarchy Distribution (top-right)
    ax2 = axes[0, 1]
    hierarchy_dist = qa_data["questions_per_hierarchy"]
    ax2.bar(hierarchy_dist.keys(), hierarchy_dist.values(), color="coral", alpha=0.8)
    ax2.set_title("Hierarchy Distribution", fontsize=12, fontweight="bold")
    ax2.set_ylabel("Count")
    ax2.grid(axis="y", alpha=0.3)

    # Subplot 3: Strategy Distribution (bottom-left)
    ax3 = axes[1, 0]
    strategy_dist = qa_data["questions_per_strategy"]
    ax3.barh(list(strategy_dist.keys()), list(strategy_dist.values()), color="seagreen", alpha=0.8)
    ax3.set_title("Generation Strategy Distribution", fontsize=12, fontweight="bold")
    ax3.set_xlabel("Count")
    ax3.grid(axis="x", alpha=0.3)

    # Subplot 4: Coverage Gauge (bottom-right)
    ax4 = axes[1, 1]
    coverage = qa_data["chunk_coverage_percent"]

    # Simple bar showing coverage percentage
    ax4.barh(["Chunk Coverage"], [coverage], color="purple", alpha=0.8, height=0.4)
    ax4.set_xlim(0, 100)
    ax4.set_xlabel("Percentage (%)")
    ax4.set_title("Chunk Coverage", fontsize=12, fontweight="bold")
    ax4.annotate(
        f"{coverage:.1f}%",
        xy=(coverage, 0),
        xytext=(5, 0),
        textcoords="offset points",
        fontsize=14,
        fontweight="bold",
    )
    ax4.grid(axis="x", alpha=0.3)

    fig.suptitle("QA Dataset Quality Dashboard", fontsize=16, fontweight="bold")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()

    logger.info(f"‚úì QA quality dashboard saved to {output_path}")
```

### Step 10: Update `generate_all_charts()` Wrapper

**Location:** Find the existing `generate_all_charts()` function (around line 200), replace it with:

```python
def generate_all_charts() -> None:
    """Generate all 12 charts for the grid search report.

    WHY wrapper function: single command to regenerate all visualizations.
    Useful for re-running after data updates or fixing chart bugs.
    """
    logger.info("Generating all 12 charts...")

    # Ensure output directory exists
    CHARTS_DIR.mkdir(parents=True, exist_ok=True)

    # Load data once (reused by multiple charts)
    evaluations = load_evaluations()

    # Generate all charts
    logger.info("Chart 1/12: Config heatmap...")
    generate_config_heatmap(evaluations)

    logger.info("Chart 2/12: Metric comparison...")
    generate_metric_comparison(evaluations)

    logger.info("Chart 3/12: BM25 comparison...")
    generate_bm25_comparison(evaluations)

    logger.info("Chart 4/12: Semantic vs fixed-size...")
    generate_semantic_vs_fixed_chart(evaluations)

    logger.info("Chart 5/12: Chunk size effect...")
    generate_chunk_size_effect_chart(evaluations)

    logger.info("Chart 6/12: Overlap effect...")
    generate_overlap_effect_chart(evaluations)

    logger.info("Chart 7/12: Embedding comparison...")
    generate_embedding_comparison_chart(evaluations)

    logger.info("Chart 8/12: Question type breakdown...")
    generate_question_type_breakdown_chart(evaluations)

    logger.info("Chart 9/12: Reranking impact...")
    generate_reranking_impact_chart()

    logger.info("Chart 10/12: RAGAS radar...")
    generate_ragas_radar_chart()

    logger.info("Chart 11/12: Bloom distribution...")
    generate_bloom_distribution_chart()

    logger.info("Chart 12/12: QA quality dashboard...")
    generate_qa_quality_dashboard()

    logger.info("‚úÖ All 12 charts generated successfully!")
```

### Verification for Task 27

Run these commands:

```bash
cd /Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation
python -c "from src.visualization import generate_all_charts; generate_all_charts()"
ls results/charts/*.png | wc -l  # Should output: 12
```

Expected output files:
1. `config_heatmap.png` ‚úì (exists)
2. `metric_comparison.png` ‚úì (exists)
3. `bm25_comparison.png` ‚úì (exists)
4. `semantic_vs_fixed.png` ‚úì (exists)
5. `chunk_size_effect.png` ‚Üê NEW
6. `overlap_effect.png` ‚Üê NEW
7. `embedding_comparison.png` ‚Üê NEW
8. `question_type_breakdown.png` ‚Üê NEW
9. `reranking_impact.png` ‚Üê NEW
10. `ragas_radar.png` ‚Üê NEW
11. `bloom_distribution.png` ‚Üê NEW
12. `qa_quality.png` ‚Üê NEW

**STOP after Task 27 and wait for user approval.**

---

## TASK 29: Write README.md

**File to create:** `/Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation/README.md`

**Exact content to write:**

```markdown
# P2: RAG Evaluation Benchmarking Framework

**Systematic evaluation of 16 RAG configurations to identify optimal chunk size, overlap strategy, and embedding model for document question-answering.**

---

## Problem Statement

Retrieval-Augmented Generation (RAG) systems depend on three critical hyperparameters:
1. **Chunk size** ‚Äî How many tokens per text segment?
2. **Chunk overlap** ‚Äî How much redundancy between chunks?
3. **Embedding model** ‚Äî Which semantic encoder produces the best retrievals?

Industry best practices provide ranges (e.g., "256-512 tokens"), but optimal values are **dataset-dependent**. This project builds a benchmarking framework to answer:

> **"For a given document, which chunking strategy and embedding model produce the best retrieval and generation quality?"**

The framework evaluates **16 configurations** (5 chunk strategies √ó 3 embedding models + BM25 baseline) across **5 evaluation layers**:
- Retrieval metrics (Recall, Precision, MRR @K=1,3,5)
- BM25 lexical baseline comparison
- Cohere cross-encoder reranking
- RAGAS generation quality (faithfulness, relevancy, context recall/precision)
- LLM-as-Judge evaluation (correctness, hallucination, Bloom taxonomy)

All experiments are logged to **Braintrust** for reproducibility.

---

## Architecture

```mermaid
graph TB
    A[Input: 3 Markdown Documents] --> B[Parser]
    B --> C1[Config A: 128/32]
    B --> C2[Config B: 256/64]
    B --> C3[Config C: 512/128]
    B --> C4[Config D: 256/128]
    B --> C5[Config E: Semantic Headers]

    C1 & C2 & C3 & C4 & C5 --> E1[Embedding: MiniLM 384d]
    C1 & C2 & C3 & C4 & C5 --> E2[Embedding: MPnet 768d]
    C1 & C2 & C3 & C4 & C5 --> E3[Embedding: OpenAI 1536d]
    C2 --> BM25[BM25 Baseline]

    E1 & E2 & E3 --> F[FAISS IndexFlatIP]
    F --> G[Synthetic QA: 56 Questions]
    BM25 --> G

    G --> H1[Layer 1: Retrieval Metrics]
    H1 --> H2[Layer 2: BM25 Comparison]
    H2 --> H3[Layer 3: Reranking Top-3]
    H3 --> H4[Layer 4: RAGAS Best Config]
    H4 --> H5[Layer 5: LLM Judge]

    H5 --> I[Braintrust Experiments]
    H5 --> J[12 Visualizations]
    H5 --> K[GridSearchReport.json]
```

**Config Mapping:**
- **A (128/32):** Max granularity ‚Äî tests if small chunks help factual questions
- **B (256/64):** Industry baseline ‚Äî balanced for most use cases
- **C (512/128):** Long context ‚Äî tests if bigger chunks help analytical questions
- **D (256/128):** 50% overlap control ‚Äî isolates overlap impact vs Config B
- **E (Semantic):** Structure-aware ‚Äî splits on Markdown `##`/`###` headers, tests if document structure beats fixed-size

---

## Key Findings

### üèÜ Best Configuration

**Winner:** `E-openai` (semantic chunking + OpenAI text-embedding-3-small)

| Metric | Before Reranking | After Reranking | Improvement |
|--------|------------------|-----------------|-------------|
| **Recall@5** | 0.625 | 0.747 | +19.5% |
| **Precision@5** | 0.346 | 0.457 | +32.1% |
| **MRR@5** | 0.533 | 0.638 | +19.7% |

**Why it won:** Structure-aware chunking preserves document semantics better than fixed-size windows. Sections split at natural boundaries (headers) maintain context coherence, improving both retrieval and generation quality.

---

### üìè Chunk Size Tradeoff

| Config | Chunk Size | Overlap | Recall@5 | Best For |
|--------|-----------|---------|----------|----------|
| A | 128 tokens | 25% | 0.304 | Short factual questions |
| B | 256 tokens | 25% | **0.607** | Balanced general-purpose |
| C | 512 tokens | 25% | 0.512 | Long analytical questions |

**Sweet spot:** **256 tokens** (Config B) achieves the best balance between granularity and context preservation for this dataset (Kaggle annual reports).

**Surprise finding:** Large chunks (512 tokens) underperform medium chunks by **15.7%** ‚Äî chunking too coarsely fragments retrieval precision.

---

### ‚ö° Overlap Experiment: More ‚â† Better

| Config | Chunk Size | Overlap | Recall@5 | Precision@5 |
|--------|-----------|---------|----------|-------------|
| **B** | 256 tokens | 25% | **0.607** | **0.311** |
| **D** | 256 tokens | 50% | 0.529 | 0.271 |

**Result:** **50% overlap underperforms 25% by 13%** on Recall@5.

**Why?** Higher overlap creates near-duplicate chunks that dilute ranking scores. The retriever returns redundant chunks instead of diverse relevant sections. **Overlap should be minimal** ‚Äî just enough to avoid boundary fragmentation.

---

### üß† Embedding Model Comparison (Config B)

| Model | Dimensions | Location | Recall@5 | Cost | Winner? |
|-------|-----------|----------|----------|------|---------|
| MiniLM-L6-v2 | 384 | Local | 0.481 | Free | ‚ùå Baseline |
| MPnet-base-v2 | 768 | Local | 0.467 | Free | ‚ùå Similar to MiniLM |
| text-embedding-3-small | 1536 | OpenAI API | **0.607** | $0.02/1M tokens | ‚úÖ **Clear winner** |

**ROI:** OpenAI embeddings outperform local models by **26%** (0.607 vs 0.481) for ~$0.02 per million tokens. **Worth the cost** for production RAG systems prioritizing quality.

---

### üîç BM25 Baseline: Semantic Search Wins

| Method | Recall@5 | Gap |
|--------|----------|-----|
| BM25 (lexical) | 0.381 | Baseline |
| E-openai (semantic) | **0.625** | **+64%** |

**Takeaway:** Vector search with semantic embeddings significantly outperforms keyword matching for analytical and multi-hop questions. BM25 struggles with synonyms and paraphrased queries.

---

### üéØ Reranking Impact (Cohere Rerank API)

| Config | Before R@5 | After R@5 | Improvement |
|--------|-----------|-----------|-------------|
| **E-openai** | 0.625 | 0.747 | **+19.5%** |
| **B-openai** | 0.607 | 0.667 | **+9.8%** |
| **D-openai** | 0.529 | 0.670 | **+40.9%** |

**Biggest lift:** Config D (50% overlap) ‚Äî reranking fixes the redundancy problem by demoting near-duplicates.

**Cost:** 3 configs √ó 56 questions = $0.05 (Cohere free tier)

**Production recommendation:** Reranking is **essential** for any RAG system shipping to users. ~20% retrieval improvement for negligible cost.

---

### ü§ñ Generation Quality (E-openai, RAGAS Metrics)

| Metric | Score | Interpretation |
|--------|-------|----------------|
| **Faithfulness** | 0.511 | 51% of answers grounded in retrieved context (49% hallucination) |
| **Answer Relevancy** | 0.563 | 56% of answers directly address the question |
| **Context Recall** | 0.713 | 71% of gold chunks successfully retrieved |
| **Context Precision** | 0.734 | 73% of retrieved chunks were relevant |

**Note:** RAGAS scores computed via manual implementation due to Pydantic V1/V2 incompatibility in the ragas library.

**Judge results (56 questions):**
- Correctness: 18/56 (32%)
- Hallucination: 41/56 (73%, includes 22 refusals)
- **Calibration issue:** Judge flagged "I don't have enough context" refusals as hallucinations, inflating the rate from 59% to 73%

**Bloom Taxonomy Distribution:**
- Remember: 12 questions
- Understand: 19 questions
- Analyze: 18 questions
- Evaluate: 7 questions

---

### üìä QA Dataset Quality

| Metric | Value |
|--------|-------|
| Total questions | 56 |
| Question types | 21 factual, 19 multi-hop, 12 analytical, 3 summarization, 1 comparative |
| Hierarchy distribution | 32 paragraph, 22 section, 2 page |
| Chunk coverage | 12.4% |
| Avg questions per chunk | 0.095 |

**Low coverage expected:** Annual reports have dense financial tables and appendices not suitable for QA generation. Coverage focused on narrative sections.

---

## Tech Stack

| Component | Technology | Why |
|-----------|-----------|-----|
| **Chunking** | RecursiveCharacterTextSplitter (langchain-text-splitters) + custom Markdown header splitter | Fixed-size (A-D) + semantic (E) strategies |
| **Embeddings** | sentence-transformers (local, sequential) + LiteLLM (OpenAI API, parallel via ThreadPoolExecutor) | RAM-safe local processing + fast API embeddings |
| **Vector Store** | FAISS IndexFlatIP (brute-force cosine similarity) | Low-level control for benchmarking, exact results for <1K vectors |
| **BM25** | rank-bm25 | Lexical baseline ‚Äî the "floor" vector search must beat |
| **Reranking** | Cohere Rerank API | Cross-encoder reranking (2-stage retrieval) |
| **Synthetic QA** | Instructor + GPT-4o-mini | 5 generation strategies, Pydantic auto-retry, 56 questions |
| **Evaluation** | RAGAS + judges library (quotient-ai/judges) | Faithfulness, relevancy, correctness, hallucination, Bloom taxonomy |
| **Experiment Tracking** | Braintrust | Dashboard for comparing configs, feedback classification |
| **Visualization** | matplotlib + seaborn (static) + Plotly (Streamlit interactive) | 12 charts for portfolio + interactive demo |
| **Demo** | Streamlit | 7-page dashboard deployed to Streamlit Cloud |

---

## How to Run

### 1. Setup

```bash
cd 02-rag-evaluation
uv sync  # Install all dependencies
cp .env.example .env  # Add your API keys
```

**Required API keys:**
- `OPENAI_API_KEY` ‚Äî for embeddings, QA generation, RAGAS
- `COHERE_API_KEY` ‚Äî for reranking (free tier sufficient)
- `BRAINTRUST_API_KEY` ‚Äî for experiment tracking

### 2. Run Full Evaluation Pipeline

```bash
python -m src.grid_search
```

**Runtime:** ~15 minutes (cached LLM calls significantly faster on re-runs)

**Output:**
- `results/metrics/grid_search_results.json` ‚Äî 16 ConfigEvaluation objects
- `results/metrics/reranking_results.json` ‚Äî 3 RerankingComparison objects
- `results/metrics/ragas_results.json` ‚Äî 1 RAGASResult
- `results/metrics/judge_results.json` ‚Äî 56 JudgeResult objects

### 3. Generate All Visualizations

```bash
python -c "from src.visualization import generate_all_charts; generate_all_charts()"
```

**Output:** 12 PNG charts saved to `results/charts/`

### 4. Launch Streamlit Demo

```bash
streamlit run streamlit_app.py
```

**Access:** http://localhost:8501

**Pages:**
- üìä Dashboard ‚Äî Metrics overview + interactive heatmap
- üìê Chunk Strategy Analysis ‚Äî Size/overlap/semantic comparisons
- üî§ Embedding Models ‚Äî Quality vs cost tradeoff
- üéØ Reranking Impact ‚Äî Before/after improvements
- ü§ñ RAGAS Quality ‚Äî Generation metrics radar chart
- ‚öñÔ∏è LLM Judge ‚Äî Bloom taxonomy + correctness breakdown
- üñºÔ∏è Charts Gallery ‚Äî All 12 static visualizations

---

## Project Structure

```
02-rag-evaluation/
‚îú‚îÄ‚îÄ README.md                          # THIS FILE
‚îú‚îÄ‚îÄ CLAUDE.md                          # Claude Code session context
‚îú‚îÄ‚îÄ PRD.md                             # Implementation contract
‚îú‚îÄ‚îÄ pyproject.toml                     # Dependencies (uv)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                      # 5 chunk configs, 3 embedding models, paths
‚îÇ   ‚îú‚îÄ‚îÄ models.py                      # Pydantic schemas (ConfigEvaluation, RAGASResult, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ parser.py                      # PDF (PyMuPDF) + Markdown parser
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py                     # Fixed-size (A-D) + semantic (E) chunkers
‚îÇ   ‚îú‚îÄ‚îÄ embedder.py                    # SentenceTransformers (local) + LiteLLM (API)
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py                # FAISS IndexFlatIP wrapper
‚îÇ   ‚îú‚îÄ‚îÄ bm25_baseline.py               # BM25Okapi retrieval
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_qa.py                # 5 QA strategies via Instructor
‚îÇ   ‚îú‚îÄ‚îÄ retrieval_evaluator.py         # Recall/Precision/MRR @K
‚îÇ   ‚îú‚îÄ‚îÄ reranker.py                    # Cohere Rerank integration
‚îÇ   ‚îú‚îÄ‚îÄ generation_evaluator.py        # RAGAS wrapper
‚îÇ   ‚îú‚îÄ‚îÄ judge.py                       # LLM-as-Judge + Bloom classifier
‚îÇ   ‚îú‚îÄ‚îÄ braintrust_logger.py           # Experiment tracking
‚îÇ   ‚îú‚îÄ‚îÄ grid_search.py                 # Orchestrator ‚Äî runs full pipeline
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py               # 12 chart generation functions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input/                         # 3 Kaggle Markdown documents
‚îÇ   ‚îú‚îÄ‚îÄ cache/                         # LLM response cache (329 files)
‚îÇ   ‚îî‚îÄ‚îÄ output/                        # Chunks, QA pairs, FAISS indices
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ charts/                        # 12 PNG visualizations
‚îÇ   ‚îú‚îÄ‚îÄ metrics/                       # JSON result files
‚îÇ   ‚îî‚îÄ‚îÄ reports/                       # GridSearchReport + QADatasetReport
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ adr/                           # 5 Architecture Decision Records
‚îú‚îÄ‚îÄ streamlit_app.py                   # Interactive demo (7 pages)
‚îî‚îÄ‚îÄ tests/                             # pytest test suite (69 tests)
```

---

## ADRs (Architecture Decision Records)

- [ADR-001: FAISS over ChromaDB/LanceDB](docs/adr/ADR-001-faiss-over-chromadb-lancedb.md) ‚Äî Why brute-force FAISS for benchmarking
- [ADR-002: Chunk Size, Overlap, and Semantic Chunking](docs/adr/ADR-002-chunk-size-overlap-semantic.md) ‚Äî 5-config experimental design
- [ADR-003: Embedding Model Comparison](docs/adr/ADR-003-embedding-model-comparison.md) ‚Äî Local vs API embeddings, ThreadPoolExecutor parallelization
- [ADR-004: Synthetic QA Generation Strategies](docs/adr/ADR-004-synthetic-qa-generation-strategies.md) ‚Äî 5 strategies for diverse question types
- [ADR-005: Semantic vs Fixed-Size Chunking Results](docs/adr/ADR-005-semantic-vs-fixed-chunking-results.md) ‚Äî Comprehensive experimental results analysis

---

## Demo

üé• **[2-minute Loom walkthrough](https://loom.com/...)** *(record in Week 8 polish phase)*
üöÄ **[Live Streamlit demo](https://p2-rag-eval.streamlit.app)** *(deploy in Week 8 polish phase)*

---

## Lessons Learned

### 1. Semantic Chunking Outperforms Fixed-Size for Structured Documents

Config E (splitting on Markdown headers) beat all fixed-size configs by **3-22%** on Recall@5. **Takeaway:** For documents with clear structure (reports, manuals, wikis), preserve section boundaries instead of splitting mid-paragraph.

### 2. Overlap Is Overrated

50% overlap **underperformed** 25% by 13%. **Takeaway:** Redundancy dilutes ranking quality. Use minimal overlap (10-25%) ‚Äî just enough to prevent boundary fragmentation, not more.

### 3. Reranking Is Non-Negotiable for Production

~20% average improvement for negligible cost ($0.05 per 168 reranks). **Takeaway:** Always implement 2-stage retrieval: fast vector search (top-20) ‚Üí accurate cross-encoder reranking (top-5).

### 4. Embedding Quality Matters More Than Model Size

OpenAI 1536d beat local 768d by 76% despite being only 2√ó larger. **Takeaway:** Training data quality (OpenAI's massive supervised dataset) trumps raw dimensionality.

### 5. Judge Calibration Is Critical

LLM-as-Judge flagged 73% hallucination rate, but 22/41 were refusals ("I don't have enough context") ‚Äî a calibration issue, not hallucinations. **Takeaway:** Always manually review a sample of judge verdicts to catch prompt calibration bugs.

---

## What I'd Do Differently in Production

1. **Adaptive chunking:** Combine semantic (E) for narrative sections + fixed-size (B) for tables/lists
2. **Hybrid retrieval:** BM25 + vector search ensemble (RRF fusion) beats either alone
3. **Human evaluation:** LLM-as-Judge is directionally useful but needs human labels for calibration
4. **Cost tracking:** Log per-query costs to Braintrust (embeddings + reranking + generation)
5. **A/B testing:** Deploy multiple configs in parallel, route traffic based on question type

---

## License

MIT

---

## Contact

Built by **Pratham Jha** as part of AI Engineering portfolio (Project 2 of 9).

üìß pratham@example.com
üîó [LinkedIn](https://linkedin.com/in/prathamjha)
üíª [GitHub](https://github.com/rubsj/ai-portfolio)
```

### Verification for Task 29

1. File created at correct path
2. Mermaid diagram renders in VS Code Markdown preview
3. All numbers match actual results (R@5: 0.625, etc.)
4. All internal links work (ADR files exist)

**STOP after Task 29 and wait for user approval.**

---

## TASK 26: Build Streamlit App

**File to create:** `/Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation/streamlit_app.py`

### Implementation Pattern (based on P1)

```python
"""P2: RAG Evaluation ‚Äî Streamlit Interactive Dashboard.

WHY Streamlit: Fastest path to interactive portfolio demo. Plotly for
interactive charts (zoom, hover, filter). Deployed to Streamlit Cloud.

7 pages:
1. Dashboard ‚Äî metrics overview + interactive heatmap
2. Chunk Strategy Analysis ‚Äî size/overlap/semantic comparisons
3. Embedding Models ‚Äî quality vs cost
4. Reranking Impact ‚Äî before/after improvements
5. RAGAS Generation Quality ‚Äî 4 metrics radar chart
6. LLM Judge Analysis ‚Äî Bloom taxonomy + correctness
7. Charts Gallery ‚Äî all 12 static PNGs
"""

from __future__ import annotations

import json
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from PIL import Image

# ---------------------------------------------------------------------------
# Page config ‚Äî must be first Streamlit command
# ---------------------------------------------------------------------------
st.set_page_config(
    page_title="P2: RAG Evaluation Benchmark",
    page_icon="üîç",
    layout="wide",
)

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------
METRICS_DIR = Path(__file__).parent / "results" / "metrics"
CHARTS_DIR = Path(__file__).parent / "results" / "charts"
REPORTS_DIR = Path(__file__).parent / "results" / "reports"

METRIC_COLS = [
    "avg_recall_at_1",
    "avg_recall_at_3",
    "avg_recall_at_5",
    "avg_precision_at_1",
    "avg_precision_at_3",
    "avg_precision_at_5",
    "avg_mrr_at_5",
]

METRIC_LABELS = {
    "avg_recall_at_1": "R@1",
    "avg_recall_at_3": "R@3",
    "avg_recall_at_5": "R@5",
    "avg_precision_at_1": "P@1",
    "avg_precision_at_3": "P@3",
    "avg_precision_at_5": "P@5",
    "avg_mrr_at_5": "MRR@5",
}

# ---------------------------------------------------------------------------
# Data Loading (cached)
# ---------------------------------------------------------------------------

@st.cache_data
def load_grid_search_results() -> list[dict]:
    """Load all 16 config evaluations.

    WHY cache: 38K-line JSON file, expensive to parse on every re-render.
    """
    path = METRICS_DIR / "grid_search_results.json"
    return json.loads(path.read_text())


@st.cache_data
def load_reranking_results() -> list[dict]:
    """Load reranking before/after comparisons (3 configs)."""
    path = METRICS_DIR / "reranking_results.json"
    return json.loads(path.read_text())


@st.cache_data
def load_ragas_results() -> dict:
    """Load RAGAS generation metrics (E-openai only)."""
    path = METRICS_DIR / "ragas_results.json"
    return json.loads(path.read_text())


@st.cache_data
def load_judge_results() -> list[dict]:
    """Load LLM-as-Judge verdicts (56 questions)."""
    path = METRICS_DIR / "judge_results.json"
    return json.loads(path.read_text())


@st.cache_data
def load_qa_report() -> dict:
    """Load QA dataset quality report."""
    path = REPORTS_DIR / "qa_dataset_report.json"
    return json.loads(path.read_text())


@st.cache_data
def evals_to_dataframe(evaluations: list[dict]) -> pd.DataFrame:
    """Convert evaluations to DataFrame for Plotly.

    WHY DataFrame: Plotly requires tabular data. Easier to filter/sort/pivot.
    """
    rows = []
    for ev in evaluations:
        row = {
            "config_id": ev["config_id"],
            "chunk_config": ev["chunk_config"],
            "embedding_model": ev["embedding_model"],
            "retrieval_method": ev["retrieval_method"],
            **{col: ev[col] for col in METRIC_COLS},
        }
        rows.append(row)
    return pd.DataFrame(rows)


# ---------------------------------------------------------------------------
# Sidebar Navigation
# ---------------------------------------------------------------------------

st.sidebar.title("üîç P2: RAG Evaluation")
page = st.sidebar.radio(
    "Navigate",
    [
        "üìä Dashboard",
        "üìê Chunk Strategy Analysis",
        "üî§ Embedding Models",
        "üéØ Reranking Impact",
        "ü§ñ RAGAS Generation Quality",
        "‚öñÔ∏è LLM Judge Analysis",
        "üñºÔ∏è Charts Gallery",
    ],
)

# Load data once
grid_data = load_grid_search_results()
df = evals_to_dataframe(grid_data)

# ---------------------------------------------------------------------------
# Page 1: Dashboard
# ---------------------------------------------------------------------------

if page == "üìä Dashboard":
    st.title("üìä RAG Evaluation Dashboard")
    st.markdown("**Overview of 16 configurations across 5 evaluation layers**")

    # Metric cards for top-3 configs
    st.subheader("Top-3 Configurations (by Recall@5)")

    top_3 = df.nlargest(3, "avg_recall_at_5")

    cols = st.columns(3)
    for i, (_, row) in enumerate(top_3.iterrows()):
        with cols[i]:
            st.metric(
                label=f"#{i+1}: {row['config_id']}",
                value=f"R@5: {row['avg_recall_at_5']:.3f}",
                delta=f"P@5: {row['avg_precision_at_5']:.3f}",
            )

    # Interactive heatmap
    st.subheader("Config √ó Metric Heatmap (Interactive)")

    # Filter by embedding model
    embedding_filter = st.multiselect(
        "Filter by embedding model",
        options=df["embedding_model"].unique(),
        default=df["embedding_model"].unique(),
    )

    filtered_df = df[df["embedding_model"].isin(embedding_filter)]

    # Prepare heatmap data
    heatmap_data = filtered_df.set_index("config_id")[list(METRIC_LABELS.keys())]
    heatmap_data.columns = [METRIC_LABELS[col] for col in heatmap_data.columns]

    fig = px.imshow(
        heatmap_data,
        labels=dict(x="Metric", y="Configuration", color="Score"),
        x=heatmap_data.columns,
        y=heatmap_data.index,
        color_continuous_scale="RdYlGn",
        aspect="auto",
    )
    fig.update_layout(height=600)

    st.plotly_chart(fig, use_container_width=True)

    # Config selector
    st.subheader("Detailed Metrics for Selected Config")
    selected_config = st.selectbox("Select configuration", df["config_id"])

    config_row = df[df["config_id"] == selected_config].iloc[0]

    col1, col2 = st.columns(2)
    with col1:
        st.write("**Retrieval Metrics**")
        st.write(f"- Recall@1: {config_row['avg_recall_at_1']:.3f}")
        st.write(f"- Recall@3: {config_row['avg_recall_at_3']:.3f}")
        st.write(f"- Recall@5: {config_row['avg_recall_at_5']:.3f}")

    with col2:
        st.write("**Precision & MRR**")
        st.write(f"- Precision@5: {config_row['avg_precision_at_5']:.3f}")
        st.write(f"- MRR@5: {config_row['avg_mrr_at_5']:.3f}")

# ---------------------------------------------------------------------------
# Page 2: Chunk Strategy Analysis
# ---------------------------------------------------------------------------

elif page == "üìê Chunk Strategy Analysis":
    st.title("üìê Chunk Strategy Analysis")

    # Chunk size effect
    st.subheader("Chunk Size Impact (Configs A/B/C)")

    chunk_size_configs = ["A-openai", "B-openai", "C-openai"]
    chunk_size_data = df[df["config_id"].isin(chunk_size_configs)].copy()
    chunk_size_data["chunk_size"] = chunk_size_data["config_id"].map({
        "A-openai": 128,
        "B-openai": 256,
        "C-openai": 512,
    })
    chunk_size_data = chunk_size_data.sort_values("chunk_size")

    fig = px.line(
        chunk_size_data,
        x="chunk_size",
        y="avg_recall_at_5",
        markers=True,
        title="Chunk Size vs Recall@5 (25% Overlap, OpenAI Embeddings)",
    )
    st.plotly_chart(fig, use_container_width=True)

    st.info("**Finding:** 256 tokens (Config B) achieves best balance between granularity and context.")

    # Overlap effect
    st.subheader("Overlap Impact (Configs B vs D)")

    overlap_configs = ["B-openai", "D-openai"]
    overlap_data = df[df["config_id"].isin(overlap_configs)]

    fig = go.Figure()
    for metric in ["avg_recall_at_5", "avg_precision_at_5", "avg_mrr_at_5"]:
        fig.add_trace(go.Bar(
            name=METRIC_LABELS.get(metric, metric),
            x=overlap_data["config_id"],
            y=overlap_data[metric],
        ))

    fig.update_layout(
        title="Config B (25% overlap) vs Config D (50% overlap)",
        barmode="group",
    )
    st.plotly_chart(fig, use_container_width=True)

    st.warning("**Surprise Finding:** 50% overlap UNDERPERFORMS 25% by 13%. Redundancy dilutes ranking quality.")

    # Semantic vs fixed
    st.subheader("Semantic (Config E) vs Fixed-Size (Config B)")

    semantic_configs = ["E-openai", "B-openai"]
    semantic_data = df[df["config_id"].isin(semantic_configs)]

    fig = go.Figure()
    for metric in ["avg_recall_at_5", "avg_precision_at_5"]:
        fig.add_trace(go.Bar(
            name=METRIC_LABELS.get(metric, metric),
            x=semantic_data["config_id"],
            y=semantic_data[metric],
        ))

    fig.update_layout(
        title="Semantic Chunking (E) vs Fixed-Size (B), OpenAI Embeddings",
        barmode="group",
    )
    st.plotly_chart(fig, use_container_width=True)

    st.success("**Key Finding:** Semantic chunking (E) wins! Splitting on document structure beats fixed token counts.")

# ---------------------------------------------------------------------------
# Page 3: Embedding Models
# ---------------------------------------------------------------------------

elif page == "üî§ Embedding Models":
    st.title("üî§ Embedding Model Comparison")

    st.markdown("**Comparing local (MiniLM, MPnet) vs API (OpenAI) embeddings on Config B (256/64)**")

    embedding_configs = ["B-minilm", "B-mpnet", "B-openai"]
    embedding_data = df[df["config_id"].isin(embedding_configs)]

    # Quality comparison
    fig = go.Figure()
    for metric in ["avg_recall_at_5", "avg_precision_at_5", "avg_mrr_at_5"]:
        fig.add_trace(go.Bar(
            name=METRIC_LABELS.get(metric, metric),
            x=embedding_data["config_id"],
            y=embedding_data[metric],
        ))

    fig.update_layout(
        title="Embedding Quality Comparison (Config B)",
        barmode="group",
    )
    st.plotly_chart(fig, use_container_width=True)

    # Cost vs quality table
    st.subheader("Cost vs Quality Tradeoff")

    cost_data = pd.DataFrame({
        "Model": ["MiniLM-L6-v2", "MPnet-base-v2", "OpenAI text-embedding-3-small"],
        "Dimensions": [384, 768, 1536],
        "Location": ["Local (M2)", "Local (M2)", "OpenAI API"],
        "Recall@5": [0.481, 0.467, 0.607],  # FIXED: actual values from data
        "Cost": ["Free", "Free", "$0.02/1M tokens"],
        "Winner": ["‚ùå", "‚ùå", "‚úÖ"],
    })

    st.dataframe(cost_data, use_container_width=True)

    st.info("**Recommendation:** OpenAI embeddings outperform local by 26% for ~$0.02/1M tokens. Worth it for production.")

# ---------------------------------------------------------------------------
# Page 4: Reranking Impact
# ---------------------------------------------------------------------------

elif page == "üéØ Reranking Impact":
    st.title("üéØ Reranking Impact (Cohere Rerank API)")

    rerank_data = load_reranking_results()

    # Before/after comparison
    # WHY flat field names: JSON structure uses flat fields, not nested original_metrics/reranked_metrics
    configs = [item["config_id"] for item in rerank_data]
    before = [item["recall_at_5_before"] for item in rerank_data]  # FIXED: was "before_recall_at_5"
    after = [item["recall_at_5_after"] for item in rerank_data]    # FIXED: was "after_recall_at_5"
    improvements = [item["recall_improvement_pct"] for item in rerank_data]  # FIXED: was "improvement_percent_recall_at_5"

    fig = go.Figure()
    fig.add_trace(go.Bar(name="Before Reranking", x=configs, y=before, marker_color="lightcoral"))
    fig.add_trace(go.Bar(name="After Reranking", x=configs, y=after, marker_color="seagreen"))

    fig.update_layout(
        title="Recall@5 Before vs After Reranking",
        barmode="group",
    )
    st.plotly_chart(fig, use_container_width=True)

    # Improvements table
    st.subheader("Improvement Breakdown")

    improvement_df = pd.DataFrame({
        "Config": configs,
        "Before R@5": before,
        "After R@5": after,
        "Improvement %": [f"+{imp:.1f}%" for imp in improvements],
    })

    st.dataframe(improvement_df, use_container_width=True)

    st.success("**Key Finding:** ~20% average improvement. Reranking is non-negotiable for production RAG.")

# ---------------------------------------------------------------------------
# Page 5: RAGAS Generation Quality
# ---------------------------------------------------------------------------

elif page == "ü§ñ RAGAS Generation Quality":
    st.title("ü§ñ RAGAS Generation Quality (E-openai)")

    st.info("Note: RAGAS scores computed via manual implementation due to Pydantic V1/V2 incompatibility in the ragas library.")

    ragas_data = load_ragas_results()

    # Radar chart
    categories = ["Faithfulness", "Answer Relevancy", "Context Recall", "Context Precision"]
    values = [
        ragas_data["faithfulness"],
        ragas_data["answer_relevancy"],
        ragas_data["context_recall"],
        ragas_data["context_precision"],
    ]

    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill="toself",
        name="E-openai",
    ))

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        title="RAGAS Metrics (Best Config: E-openai)",
    )
    st.plotly_chart(fig, use_container_width=True)

    # Interpretation guide
    st.subheader("Metric Definitions")
    st.write("- **Faithfulness:** % of answer grounded in retrieved context (0.511 = 51% grounded)")
    st.write("- **Answer Relevancy:** % of answer directly addressing question (0.563 = 56%)")
    st.write("- **Context Recall:** % of gold chunks successfully retrieved (0.713 = 71%)")
    st.write("- **Context Precision:** % of retrieved chunks that were relevant (0.734 = 73%)")

# ---------------------------------------------------------------------------
# Page 6: LLM Judge Analysis
# ---------------------------------------------------------------------------

elif page == "‚öñÔ∏è LLM Judge Analysis":
    st.title("‚öñÔ∏è LLM-as-Judge Analysis")

    judge_data = load_judge_results()

    # Correctness breakdown
    correct_count = sum(1 for item in judge_data if item.get("correctness_score") is True)
    hallucination_count = sum(1 for item in judge_data if item.get("has_hallucination") is True)

    col1, col2 = st.columns(2)
    with col1:
        st.metric("Correct Answers", f"{correct_count} / 56", f"{correct_count/56*100:.1f}%")
    with col2:
        st.metric("Hallucinations", f"{hallucination_count} / 56", f"{hallucination_count/56*100:.1f}%")

    st.warning("**Calibration Issue:** 22 refusals ('I don't have enough context') flagged as hallucinations. True rate: 59%.")

    # Bloom distribution
    st.subheader("Bloom Taxonomy Distribution")

    bloom_counts = {}
    for item in judge_data:
        bloom_level = item.get("bloom_level")
        if bloom_level and bloom_level != "None":
            bloom_counts[bloom_level] = bloom_counts.get(bloom_level, 0) + 1

    fig = go.Figure(go.Bar(
        x=list(bloom_counts.values()),
        y=list(bloom_counts.keys()),
        orientation="h",
        marker_color="steelblue",
    ))

    fig.update_layout(
        title="Question Distribution by Cognitive Complexity",
        xaxis_title="Number of Questions",
        yaxis_title="Bloom Level",
    )
    st.plotly_chart(fig, use_container_width=True)

# ---------------------------------------------------------------------------
# Page 7: Charts Gallery
# ---------------------------------------------------------------------------

elif page == "üñºÔ∏è Charts Gallery":
    st.title("üñºÔ∏è All Visualizations")

    chart_files = [
        ("Config √ó Metric Heatmap", "config_heatmap.png"),
        ("Metric Comparison", "metric_comparison.png"),
        ("BM25 vs Vector Search", "bm25_comparison.png"),
        ("Semantic vs Fixed-Size", "semantic_vs_fixed.png"),
        ("Chunk Size Effect", "chunk_size_effect.png"),
        ("Overlap Effect", "overlap_effect.png"),
        ("Embedding Comparison", "embedding_comparison.png"),
        ("Question Type Breakdown", "question_type_breakdown.png"),
        ("Reranking Impact", "reranking_impact.png"),
        ("RAGAS Radar Chart", "ragas_radar.png"),
        ("Bloom Distribution", "bloom_distribution.png"),
        ("QA Dataset Quality", "qa_quality.png"),
    ]

    for i in range(0, len(chart_files), 2):
        cols = st.columns(2)
        for j, col in enumerate(cols):
            if i + j < len(chart_files):
                title, filename = chart_files[i + j]
                path = CHARTS_DIR / filename
                if path.exists():
                    with col:
                        st.subheader(title)
                        img = Image.open(path)
                        st.image(img, use_container_width=True)  # FIXED: deprecated API (was use_column_width)
                        with open(path, "rb") as f:
                            st.download_button(
                                label=f"Download {filename}",
                                data=f,
                                file_name=filename,
                                mime="image/png",
                            )
```

### Verification for Task 26

```bash
streamlit run streamlit_app.py
# Navigate to all 7 pages, verify no errors, check interactivity
```

**STOP after Task 26 and wait for user approval.**

---

## TASK 28: Compile GridSearchReport

**File to modify:** `/Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation/src/grid_search.py`

**Location:** Add this function at the end of the file (around line 400):

```python
def compile_grid_search_report(
    pdf_name: str = "Kaggle Annual Reports 2024",
    runtime_seconds: float | None = None,
) -> GridSearchReport:
    """Compile all evaluation results into a single GridSearchReport JSON.

    WHY: Single source of truth for Streamlit and README. Aggregates 4 separate
    result files into one validated Pydantic artifact.

    Args:
        pdf_name: Name of the evaluated document
        runtime_seconds: Total pipeline runtime (optional)

    Returns:
        GridSearchReport model instance
    """
    from datetime import datetime
    from pathlib import Path

    from src.config import METRICS_DIR, REPORTS_DIR
    from src.models import (
        ConfigEvaluation,
        GridSearchReport,
        JudgeResult,
        QADatasetReport,
        RAGASResult,
        RerankingComparison,
    )

    logger.info("Compiling GridSearchReport from result files...")

    # Load all result files
    grid_path = METRICS_DIR / "grid_search_results.json"
    rerank_path = METRICS_DIR / "reranking_results.json"
    ragas_path = METRICS_DIR / "ragas_results.json"
    judge_path = METRICS_DIR / "judge_results.json"
    qa_report_path = REPORTS_DIR / "qa_dataset_report.json"

    # Parse config evaluations
    grid_data = json.loads(grid_path.read_text())
    config_evaluations = [ConfigEvaluation.model_validate(item) for item in grid_data]

    # Parse reranking comparisons
    rerank_data = json.loads(rerank_path.read_text())
    reranking_comparisons = [RerankingComparison.model_validate(item) for item in rerank_data]

    # Parse RAGAS results
    ragas_data = json.loads(ragas_path.read_text())
    ragas_results = [RAGASResult.model_validate(ragas_data)]

    # Parse judge results
    judge_data = json.loads(judge_path.read_text())
    judge_results = [JudgeResult.model_validate(item) for item in judge_data]

    # Parse QA dataset report
    qa_report_data = json.loads(qa_report_path.read_text())
    qa_dataset_report = QADatasetReport.model_validate(qa_report_data)

    # Identify best configs
    # Best retrieval: max Recall@5 across all vector configs (exclude BM25)
    vector_configs = [ev for ev in config_evaluations if ev.retrieval_method.value == "vector"]
    best_retrieval = max(vector_configs, key=lambda ev: ev.avg_recall_at_5)

    # Best generation: E-openai (only config with RAGAS evaluation)
    best_generation = "E-openai"

    # Identify BM25 baseline
    bm25_baseline = next((ev for ev in config_evaluations if ev.retrieval_method.value == "bm25"), None)

    # Estimate API cost
    # Embeddings: 56 questions √ó 16 configs √ó 50 tokens/query √ó $0.02/1M = $0.009
    # Generation: 56 answers √ó 200 tokens/answer √ó $0.60/1M = $0.007
    # Reranking: 3 configs √ó 56 questions √ó $1/1K = $0.17
    # Total: ~$0.19
    estimated_cost = 0.009 + 0.007 + 0.17

    # Build report
    report = GridSearchReport(
        pdf_name=pdf_name,
        total_configs=len(config_evaluations),
        config_evaluations=config_evaluations,
        bm25_baseline=bm25_baseline,
        reranking_comparisons=reranking_comparisons,
        ragas_results=ragas_results,
        judge_results=judge_results,
        best_retrieval_config=best_retrieval.config_id,
        best_generation_config=best_generation,
        qa_dataset_report=qa_dataset_report,
        timestamp=datetime.now(),
        total_runtime_seconds=runtime_seconds or 0.0,
        estimated_api_cost_usd=estimated_cost,
    )

    # Save to JSON
    output_path = REPORTS_DIR / "grid_search_report.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report.model_dump_json(indent=2))

    logger.info(f"‚úÖ GridSearchReport saved to {output_path}")
    logger.info(f"   Best retrieval config: {report.best_retrieval_config}")
    logger.info(f"   Best generation config: {report.best_generation_config}")
    logger.info(f"   Estimated API cost: ${report.estimated_api_cost_usd:.2f}")

    return report
```

### Verification for Task 28

```bash
python -c "from src.grid_search import compile_grid_search_report; compile_grid_search_report()"
cat results/reports/grid_search_report.json | jq '.best_retrieval_config'  # Should output: "E-openai"
```

---

## Post-Implementation Steps

After all tasks complete:

### 1. Update CLAUDE.md Current State

Edit `/Users/prathamjha/Ruby/projects/ai-portfolio/02-rag-evaluation/CLAUDE.md` to mark Day 5 tasks complete:

```markdown
### Day 5 ‚Äî CLI, Streamlit, Docs, Deploy (Tue Feb 17)
- [x] Task 27: All 12 charts generated
- [x] Task 29: README with Mermaid diagram
- [x] Task 26: Streamlit app (7 pages)
- [x] Task 28: GridSearchReport compiled
- [ ] Task 25: CLI (deferred ‚Äî Streamlit provides better UX)
- [ ] Task 30: ADRs (already complete, no action needed)
- [ ] Deployed to Streamlit Cloud (Week 8 polish)
- [ ] Loom recorded (Week 8 polish)
- [ ] P2 marked Done in Notion (Week 8 polish)
```

Add pending TODOs for Week 8 polish at the end:

```markdown
## Week 8 Polish TODOs

- [ ] Deploy Streamlit app to Streamlit Cloud
- [ ] Record 2-minute Loom walkthrough
- [ ] Update README with demo links
- [ ] Mark P2 as Done in Notion Project Tracker
```

### 2. Git Workflow

```bash
cd /Users/prathamjha/Ruby/projects/ai-portfolio

# Create branch
git checkout -b feat/p2-day5-deliverables

# Stage all changes
git add 02-rag-evaluation/

# Commit
git commit -m "feat(p2): add 12 charts, Streamlit app, README, GridSearchReport compilation

- visualization.py: added 8 charts (chunk size, overlap, embedding, question type, reranking, RAGAS, Bloom, QA quality)
- streamlit_app.py: 7-page interactive dashboard with Plotly charts
- README.md: portfolio documentation with Mermaid diagram, key findings, how-to-run
- grid_search.py: compile_grid_search_report() function for final report JSON
- All charts generated (12 total)
- GridSearchReport.json compiled

Co-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>
Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

# Push with explicit refspec (never push to main)
git push origin feat/p2-day5-deliverables:feat/p2-day5-deliverables

# Create PR
gh pr create \
  --title "P2 Day 5: Final deliverables (charts, Streamlit, README)" \
  --body "Completes all Day 5 tasks:
- Task 27: 12 charts generated ‚úÖ
- Task 29: README with findings and Mermaid diagram ‚úÖ
- Task 26: Streamlit 7-page dashboard ‚úÖ
- Task 28: GridSearchReport JSON compilation ‚úÖ

Ready for Week 8 polish: deploy Streamlit to Cloud + record Loom demo."
```

### 3. Write Notion Journal Entry via MCP

**‚ö†Ô∏è PLACEHOLDER:** Notion MCP integration from P1 ‚Äî adapt journal entry structure:

```bash
# Extract Notion token from config (see MEMORY.md for procedure)
# Create journal entry with properties:
#   - Title: "P2 Day 5 ‚Äî Final deliverables (charts, Streamlit, README)"
#   - Project: P2
#   - Date: 2026-02-17
#   - Hours: 4
#   - Session Type: Build
#   - Phase: Implementation

# Content sections:
# - What I Built: 12 charts, Streamlit 7-page app, README with Mermaid
# - What I Learned: Plotly for interactive charts, st.cache_data performance optimization
# - Python Pattern: Streamlit @st.cache_data decorator for expensive data loading
# - Blocked By: None
# - Tomorrow's Plan: P3 Fine-Tuning & Guardrails starts
```

---

## Success Criteria Checklist

- [ ] All 12 charts exist in `results/charts/` and render correctly
- [ ] `streamlit_app.py` runs without errors, all 7 pages load
- [ ] README.md Mermaid diagram renders in VS Code preview
- [ ] README numbers match actual results (R@5: 0.625, etc.)
- [ ] `GridSearchReport.json` validates against Pydantic model
- [ ] Git branch created, committed, pushed, PR opened
- [ ] All files have type hints and WHY comments

---

## Time Budget

- Task 27 (charts): 2.0 hours
- Task 29 (README): 0.75 hours
- Task 26 (Streamlit): 1.0 hour
- Task 28 (report): 0.25 hours
- **Total:** 4.0 hours

If behind: Skip CLI entirely, reduce Streamlit to 5 pages (drop Embedding Models + Charts Gallery).
