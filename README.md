# AI Portfolio â€” 9 Projects in 8 Weeks

> Production-grade AI systems with measurable results, not toy demos.

![Python](https://img.shields.io/badge/Python-3.12+-3776AB?logo=python&logoColor=white)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4o-412991?logo=openai&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-0.3-1C3C3C)
![FAISS](https://img.shields.io/badge/FAISS-Meta-lightgrey)
![RAGAS](https://img.shields.io/badge/RAGAS-Eval-F97316)
![CrewAI](https://img.shields.io/badge/CrewAI-Multi--Agent-DC2626)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?logo=fastapi&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-1.x-FF4B4B?logo=streamlit&logoColor=white)
![Pydantic](https://img.shields.io/badge/Pydantic-v2-E92063)

---

## About

A focused 8-week engineering sprint (Febâ€“Apr 2026) building nine production-grade AI systems end-to-end â€” from synthetic data pipelines to multi-agent orchestration and DevOps automation. Each project targets a distinct layer of the modern AI engineering stack: synthetic data generation, RAG evaluation, embedding fine-tuning, agentic pipelines, and production deployment. Designed to demonstrate the depth and rigor expected at Tier-1 engineering organizations, not just proof-of-concept familiarity.

---

## Projects

| # | Project | Status | Key Techniques | Stack |
|---|---------|--------|----------------|-------|
| 1 | [Synthetic Data â€” Home DIY Repair](./01-synthetic-data-home-diy/) | âœ… Complete | Structured generation, LLM-as-Judge, correction loop, Jaccard similarity | Pydantic, OpenAI, Instructor, Streamlit |
| 2 | [Evaluating RAG for Any PDF](./02-rag-evaluation/) | âœ… Complete | 16-config grid search, chunking strategies, hybrid reranking, RAGAS metrics | LangChain, FAISS, RAGAS, Cohere, Braintrust, Click |
| 3 | [Contrastive Embedding Fine-Tuning](./03-fine-tuning-guardrails/) | ğŸ”„ In Progress | CosineSimilarityLoss, LoRA adapters, UMAP, HDBSCAN, Spearman correlation | Sentence-Transformers, PEFT/LoRA, OpenAI, Braintrust |
| 4 | [AI-Powered Resume Coach](./04-resume-coach/) | â³ Upcoming | JD analysis, controlled fit levels, LLM-as-Judge evaluation | OpenAI, Pydantic, FastAPI |
| 5 | [Production RAG System](./05-production-rag/) | â³ Upcoming | Multi-strategy chunking, hybrid search, Cohere reranking, REST API | LangChain, FAISS, Cohere, FastAPI, Click |
| 6 | [Digital Writing Clone â€” 5-Agent](./06-digital-writing-clone/) | â³ Upcoming | StyleAnalyzer, RAG, Evaluator, Fallback, Planner agents; style-matched generation | CrewAI, LangChain, OpenAI |
| 7 | [Customer Feedback Intelligence](./07-feedback-intelligence/) | â³ Upcoming | CrewAI pipeline: Sentiment, Theme, Mapping, Gap agents | CrewAI, OpenAI, Pydantic |
| 8 | [Jira AI Agent](./08-jira-ai-agent/) | â³ Upcoming | Semantic search, duplicate detection, sprint planning | OpenAI, FAISS, FastAPI, Click |
| 9 | [DevOps AI Assistant (Capstone)](./09-devops-assistant/) | â³ Upcoming | 5-agent system: CI/CD monitoring, log analysis, root cause, remediation | CrewAI, OpenAI, FastAPI |

---

## Completed Project Highlights

### P1 â€” Synthetic Data: Home DIY Repair

**30 structured QA records** across 5 repair categories Ã— 2 formats Ã— 3 difficulty levels.

- **100% generation success rate** with zero schema validation failures (Pydantic + Instructor)
- **LLM-as-Judge calibration**: GPT-4o judge tuned from 0% â†’ 20% failure detection via prompt engineering
- **Correction loop**: 78% failure reduction (V1 â†’ V2 templates) â€” upstream prompt improvement outperformed downstream correction
- **7 publication-quality charts**, 6-page Streamlit app with Story Mode narrative, 4 ADRs

â†’ [Project README](./01-synthetic-data-home-diy/README.md)

---

### P2 â€” Evaluating RAG for Any PDF

**Systematic benchmarking** of 16 retrieval configurations across a real PDF corpus.

- **Best config**: semantic chunking + OpenAI embeddings â†’ Recall@5 = 0.625; +19.5% with Cohere reranking (â†’ 0.747)
- **BM25 baseline beaten** by 10 of 15 vector configs; OpenAI embeddings outperform local models by 26% for $0.02/1M tokens
- **Key findings**: 50% chunk overlap underperforms 25% by 13%; faithfulness gap of 0.511 shows retrieval â‰  generation quality; 39% LLM refusal rate misclassified as hallucinations
- **95% test coverage** (384+ tests), 12 charts, 7-page Streamlit dashboard, Click CLI with Rich formatting, 5 ADRs, ~20 PRs

â†’ [Project README](./02-rag-evaluation/README.md)

---

### P3 â€” Contrastive Embedding Fine-Tuning *(In Progress)*

Fine-tuning a sentence-transformer model on dating compatibility data using **CosineSimilarityLoss + LoRA adapters** to reshape the embedding space for compatible/incompatible pair separation. Before/after evaluation via cosine similarity distributions, UMAP clustering, HDBSCAN purity, and Cohen's d. Target: Spearman correlation baseline ~0.76 â†’ â‰¥0.86 post-fine-tuning.

---

## Repository Structure

```
ai-portfolio/
â”œâ”€â”€ 01-synthetic-data-home-diy/   # P1 â€” Complete
â”‚   â”œâ”€â”€ src/                      # schemas, templates, generator, validator, evaluator, analysis, corrector
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ 02-rag-evaluation/            # P2 â€” Complete
â”‚   â”œâ”€â”€ src/                      # chunker, embedder, index_builder, retrieval_evaluator, judge, reranker, cli, ...
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ streamlit_app.py
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ 03-fine-tuning-guardrails/    # P3 â€” In Progress
â”‚   â”œâ”€â”€ src/                      # models, data_loader, data_evaluator, metrics
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ eval/
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ 04-resume-coach/              # P4 â€” Upcoming (Week 3)
â”œâ”€â”€ 05-production-rag/            # P5 â€” Upcoming (Week 4)
â”œâ”€â”€ 06-digital-writing-clone/     # P6 â€” Upcoming (Week 4â€“5)
â”œâ”€â”€ 07-feedback-intelligence/     # P7 â€” Upcoming (Week 5)
â”œâ”€â”€ 08-jira-ai-agent/             # P8 â€” Upcoming (Week 6)
â”œâ”€â”€ 09-devops-assistant/          # P9 â€” Upcoming (Week 7)
â”œâ”€â”€ shared/                       # Cross-project utilities (llm_client, cache, eval_helpers)
â”œâ”€â”€ docs/                         # Cross-project ADRs, monthly learning notes
â””â”€â”€ CLAUDE.md                     # Persistent engineering context
```

---

## Tech Stack

| Category | Tools |
|----------|-------|
| **LLM & AI** | OpenAI API (GPT-4o-mini / GPT-4o), Sentence-Transformers, Instructor, Cohere Rerank |
| **Data & Evaluation** | RAGAS, Braintrust, FAISS, Pydantic v2 |
| **Frameworks** | LangChain, CrewAI, FastAPI |
| **Frontend & Viz** | Streamlit, Rich, Matplotlib, Seaborn |
| **DevOps & Testing** | Click CLI, pytest, ruff, uv, GitHub Actions |

---

## Engineering Practices

- **Architecture Decision Records** â€” every significant design choice documented per project (4 in P1, 5 in P2)
- **95%+ test coverage targets** â€” pytest with parametrized cases covering happy path and failure modes
- **5-layer validation methodology** â€” schema validation, semantic checks, LLM-as-Judge, correction loops, re-evaluation
- **LLM cost management** â€” MD5-keyed disk cache for all LLM calls; estimated cost logged per call
- **Clean PR history** â€” feature branches, atomic commits, ~20 PRs merged across projects
- **Reproducible results** â€” all generation seeds, model versions, and evaluation configs pinned and committed

---

Each project includes comprehensive documentation, reproducible results, and interview-ready technical narratives with quantifiable outcomes.

**Connect:** [LinkedIn](https://linkedin.com/in/yourprofile) Â· [Portfolio Site](https://yourportfolio.dev)
