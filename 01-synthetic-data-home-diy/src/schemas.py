"""Pydantic models for the synthetic DIY repair data pipeline.

Defines the data contracts for generation, evaluation, and correction.
All LLM-generated data flows through these models for structural validation.

Java/TS parallel: Think of these as TypeScript interfaces with built-in runtime
validation — like zod schemas that enforce types AND business rules at parse time.
"""

from __future__ import annotations

from typing import Annotated, Literal

from pydantic import BaseModel, Field, field_validator


# ---------------------------------------------------------------------------
# Type aliases — single source of truth for valid categories and difficulties.
# Java/TS parallel: like TypeScript union types, e.g.
#   type Category = "appliance_repair" | "plumbing_repair" | ...
# Using Literal (not Enum) because:
#   1. Instructor generates cleaner JSON schema for LLM consumption
#   2. Plain strings in JSON — no .value extraction needed
#   3. PRD specifies plain strings, not enums
# ---------------------------------------------------------------------------
Category = Literal[
    "appliance_repair",
    "plumbing_repair",
    "electrical_repair",
    "hvac_maintenance",
    "general_home_repair",
]

Difficulty = Literal["beginner", "intermediate", "advanced"]

FailureMode = Literal[
    "incomplete_answer",
    "safety_violations",
    "unrealistic_tools",
    "overcomplicated_solution",
    "missing_context",
    "poor_quality_tips",
]

# Total number of failure modes — used to validate JudgeResult.labels length
_NUM_FAILURE_MODES = 6


# ===========================================================================
# Model 1: DIYRepairRecord — what the LLM generates
# ===========================================================================
class DIYRepairRecord(BaseModel):
    """A single DIY home repair Q&A record generated by GPT-4o-mini.

    This is the response_model passed to Instructor. Instructor injects the
    JSON schema from this model into the LLM prompt and auto-validates the
    response. If validation fails, Instructor retries (up to max_retries)
    with the error message fed back to the LLM.

    7 required fields, flat structure (PRD Section 2 decision: no nesting).
    """

    question: str = Field(
        min_length=10,
        description="A clear question a homeowner would ask about a DIY repair",
    )
    answer: str = Field(
        min_length=50,
        description="Detailed step-by-step answer to the repair question",
    )
    equipment_problem: str = Field(
        min_length=5,
        description="Specific problem or equipment description",
    )
    # Annotated[str, Field(min_length=2)] validates EACH item in the list.
    # Field(min_length=1) on the list itself ensures at least one tool.
    # Java/TS parallel: like zod.array(zod.string().min(2)).min(1)
    tools_required: list[Annotated[str, Field(min_length=2)]] = Field(
        min_length=1,
        description="List of tools needed for this repair",
    )
    # Same pattern: at least 2 steps, each step at least 10 chars.
    steps: list[Annotated[str, Field(min_length=10)]] = Field(
        min_length=2,
        description="Ordered repair steps the homeowner should follow",
    )
    safety_info: str = Field(
        min_length=10,
        description="Safety warnings and precautions for this repair",
    )
    tips: str = Field(
        min_length=5,
        description="Helpful practical tips for this repair",
    )

    # Pydantic v2 pattern: @field_validator with @classmethod.
    # Java/TS parallel: like Zod's .refine() or Spring's @Valid + custom
    # ConstraintValidator. Runs AFTER type coercion, BEFORE model construction.
    @field_validator("question")
    @classmethod
    def question_must_end_with_question_mark(cls, v: str) -> str:
        """LLMs sometimes generate statements instead of questions.
        Catches this at the structural layer so Instructor retries
        with the error fed back to the LLM.
        """
        if not v.strip().endswith("?"):
            raise ValueError("Question must end with '?' — got a statement instead")
        return v


# ===========================================================================
# Model 2: GeneratedRecord — wraps DIYRepairRecord with pipeline metadata
# ===========================================================================
class GeneratedRecord(BaseModel):
    """A generated record plus metadata for tracing and caching.

    Created by generator.py AFTER Instructor returns a valid DIYRepairRecord.
    Metadata fields are populated by the pipeline, not by the LLM.

    Java/TS parallel: like a DTO that wraps the domain object with
    infrastructure concerns (traceId, timestamp, etc.).
    """

    trace_id: str = Field(
        description="UUID for tracing this record through the pipeline",
    )
    category: Category = Field(
        description="Which of the 5 repair categories this record belongs to",
    )
    difficulty: Difficulty = Field(
        description="Complexity level of the repair",
    )
    template_version: str = Field(
        description="Prompt template version ('v1' initially, 'v2' after improvement)",
    )
    generation_timestamp: str = Field(
        description="ISO 8601 datetime when this record was generated",
    )
    model_used: str = Field(
        description="LLM model identifier (e.g., 'gpt-4o-mini')",
    )
    prompt_hash: str = Field(
        description="MD5 hash of the full prompt — used as cache key",
    )
    record: DIYRepairRecord = Field(
        description="The actual generated repair Q&A data",
    )


# ===========================================================================
# Model 3: FailureLabel — one failure mode evaluation
# ===========================================================================
class FailureLabel(BaseModel):
    """A single binary label from the LLM-as-Judge for one failure mode.

    The judge evaluates each record against 6 failure modes (PRD Section 6a).
    Each mode gets a 0 (pass) or 1 (fail) plus a reason explaining the judgment.
    """

    mode: FailureMode = Field(
        description="Which failure mode this label evaluates",
    )
    label: Literal[0, 1] = Field(
        description="0 = pass (no failure detected), 1 = fail (failure detected)",
    )
    reason: str = Field(
        min_length=5,
        description="Brief explanation of why this mode passed or failed",
    )


# ===========================================================================
# Model 4: JudgeResult — full evaluation of one record
# ===========================================================================
class JudgeResult(BaseModel):
    """Complete evaluation output from the LLM-as-Judge for one record.

    Contains exactly 6 FailureLabel items (one per failure mode) plus an
    overall quality score. Used as Instructor's response_model when calling
    GPT-4o for evaluation.
    """

    trace_id: str = Field(
        description="UUID of the GeneratedRecord being evaluated",
    )
    labels: list[FailureLabel] = Field(
        min_length=_NUM_FAILURE_MODES,
        max_length=_NUM_FAILURE_MODES,
        description="Exactly 6 failure mode labels, one per mode",
    )
    overall_quality_score: int = Field(
        ge=1,
        le=5,
        description="Overall quality rating: 1 (very poor) to 5 (excellent)",
    )

    @field_validator("labels")
    @classmethod
    def all_failure_modes_present(cls, v: list[FailureLabel]) -> list[FailureLabel]:
        """Ensure the judge evaluated ALL 6 failure modes, not a subset.
        LLMs sometimes skip modes or duplicate them — this catches both.
        """
        modes_present = {label.mode for label in v}
        expected_modes = {
            "incomplete_answer",
            "safety_violations",
            "unrealistic_tools",
            "overcomplicated_solution",
            "missing_context",
            "poor_quality_tips",
        }
        missing = expected_modes - modes_present
        if missing:
            raise ValueError(
                f"Missing failure mode evaluations: {', '.join(sorted(missing))}"
            )
        return v
