# ADR-003: Judge Prompt Calibration — from 0% to 20% Failure Rate

**Date**: 2026-02-09
**Status**: Accepted
**Project**: P1 — Synthetic Data, Home DIY Repair

## Context

The LLM-as-Judge pattern (PRD Section 6) uses GPT-4o to evaluate records
generated by GPT-4o-mini across 6 binary failure modes. The judge's job is to
find real quality deficiencies — incomplete answers, missing safety warnings,
unrealistic tools, etc.

Our first judge prompt was **lenient**: it described the 6 failure modes with
one-sentence definitions and asked the model to score each as 0 (pass) or 1
(fail). Result: **0% failure rate** — the judge passed every record on every
mode. This was clearly wrong; manual inspection revealed genuine issues (e.g.,
electrical repairs without explicit "turn off the circuit breaker" as step 1,
beginner tasks requiring specialty tools).

The problem: GPT-4o defaults to "helpful and agreeable." Without explicit
instructions to be strict, it rationalizes borderline cases as passes. This is a
known failure pattern in LLM-as-Judge systems — the model's RLHF training
optimizes for helpfulness, which biases it toward positive assessments.

## Decision

Rewrite the judge system prompt with **explicit strictness calibration**. The
final prompt (in `evaluator.py`) produces a ~20% failure rate (36 failures
across 30 records × 6 modes = 180 evaluations), which aligns with manual
inspection.

### Before: Lenient Prompt (0% failure rate)

```text
You are a quality evaluator for DIY home repair guides.
Evaluate the following repair guide for these 6 failure modes.
For each mode, respond with 0 (pass) or 1 (fail) and a brief reason.

1. incomplete_answer: Answer lacks sufficient detail
2. safety_violations: Missing or inadequate safety info
3. unrealistic_tools: Tools a homeowner wouldn't have
4. overcomplicated_solution: Too complex for stated difficulty
5. missing_context: Missing equipment_problem or category context
6. poor_quality_tips: Tips are obvious or unhelpful
```

**Why it failed**: One-line definitions leave too much room for interpretation.
"Lacks sufficient detail" — by whose standard? The model defaults to its own
generous standard. No identity priming, no anchoring, no concrete thresholds.

### After: Strict Prompt (20% failure rate)

This is the production prompt in `evaluator.py:_JUDGE_SYSTEM_PROMPT`:

```text
You are a STRICT quality evaluator for DIY home repair guides.
Your job is to find deficiencies. Most guides have at least 1-2 issues.
Evaluate the following repair guide for these 6 failure modes.
For each mode, respond with 0 (pass) or 1 (fail) and a brief reason.

Failure mode definitions (apply these STRICTLY):

1. incomplete_answer: Fail if the answer has fewer than 3 substantive sentences,
omits key information a homeowner needs (e.g., how to identify the part, where to
buy replacements, what to do if the fix doesn't work), or lacks troubleshooting advice.

2. safety_violations: Fail if ANY of these are missing when relevant:
power-off/water-off warning BEFORE starting work, PPE requirements (gloves, goggles),
warning about when to call a licensed professional instead, specific hazard callouts
(e.g., electrical shock risk, gas leak risk, hot surfaces). For electrical/HVAC repairs,
ALWAYS check that circuit breaker/power disconnect is mentioned as the FIRST step.

3. unrealistic_tools: Fail if any tool is specialized enough that a typical homeowner
would need to buy or rent it specifically for this repair (e.g., multimeter, cartridge
puller, torque wrench). Also fail if the tool list is suspiciously short for the complexity
of the repair.

4. overcomplicated_solution: For beginner tasks, fail if more than 8 steps or requires
any specialty tools. For intermediate, fail if more than 12 steps. For advanced tasks,
fail if the repair should ONLY be done by a licensed professional (e.g., working inside
an electrical panel, replacing gas lines, modifying structural elements). Also fail if
the guide doesn't explicitly state when to stop and call a professional.

5. missing_context: Fail if the answer gives generic advice not specifically tied to
the equipment_problem field. The answer should reference the specific equipment/problem
described, not just give a general category overview.

6. poor_quality_tips: Fail if tips are generic platitudes ("be careful", "take your time"),
repeat information already in the steps or safety_info, or miss an opportunity to provide
genuinely useful advice specific to this repair.

Be thorough and critical. A guide that is merely "okay" should still get failures flagged.
Only give 0 (pass) when the guide is genuinely strong in that dimension.
Quality scores: 1=very poor, 2=poor, 3=adequate, 4=good, 5=excellent. Most guides are 3-4.
```

### What Changed (Line by Line)

| Element | Lenient (before) | Strict (after) | Why it matters |
|---------|-----------------|----------------|----------------|
| **Identity** | "You are a quality evaluator" | "You are a **STRICT** quality evaluator" | Primes the model to be critical, not agreeable |
| **Expectation setting** | *(none)* | "Your job is to find deficiencies. Most guides have at least 1-2 issues." | Normalizes failures — the model expects to flag things |
| **Mode definitions** | One line each (e.g., "Answer lacks sufficient detail") | Multi-sentence with concrete thresholds, examples, and edge cases | Removes interpretive wiggle room |
| **safety_violations** | "Missing or inadequate safety info" | Explicit checklist: power-off warning, PPE, professional referral, hazard callouts + "For electrical/HVAC, circuit breaker must be FIRST step" | Converts vague "inadequate" into a pass/fail checklist |
| **unrealistic_tools** | "Tools a homeowner wouldn't have" | Named examples (multimeter, cartridge puller, torque wrench) + "fail if tool list is suspiciously short" | Grounds abstract "wouldn't have" in concrete items |
| **overcomplicated** | "Too complex for stated difficulty" | Hard step-count limits: beginner ≤ 8, intermediate ≤ 12. Advanced: fail if should ONLY be done by a pro | Numeric thresholds eliminate subjectivity |
| **Anti-rationalization** | *(none)* | "A guide that is merely 'okay' should still get failures flagged. Only give 0 when genuinely strong." | Directly combats the model's positivity bias |
| **Score anchoring** | *(none)* | "Most guides are 3-4" | Prevents score inflation (without this, most scores cluster at 4-5) |

## Alternatives Considered

| Option | Pros | Cons |
|--------|------|------|
| **Lenient prompt** (original) | Simple, few failures to analyze | 0% failure rate is useless — defeats the purpose of evaluation |
| **Strict prompt with concrete criteria** (chosen) | 20% failure rate matches manual review, reproducible via caching, actionable failure reasons | Requires domain expertise to write good criteria, may need re-calibration per domain in future projects |
| **Multi-turn judge** (ask, then challenge) | Could catch more edge cases | 2x API cost per record, more complex orchestration, diminishing returns on 30 records |
| **Temperature tuning only** (lower temp = stricter?) | No prompt rewrite needed | Temperature controls randomness, not strictness. Low temperature (0.3) makes the model more consistent, not more critical |

## Consequences

**Easier:**
- The 20% failure rate gives the correction loop (corrector.py) meaningful work
  to do — without failures, there's nothing to correct and the pipeline is
  trivially boring.
- Failure reasons in the `JudgeResult.labels[].reason` field are specific enough
  to drive targeted prompt improvements (v2 templates).
- The calibration pattern (identity priming + concrete criteria + anti-
  rationalization anchor) is portable to P2–P9 judge prompts.

**Harder:**
- The "right" failure rate is subjective. 20% could be too strict or too lenient
  depending on the reviewer. Manual labeling (next step) will validate whether
  the LLM judge's failures match human judgment.
- Criteria are domain-specific (step counts, tool lists, safety checks). Each
  new project will need its own calibrated criteria, not a copy-paste of this
  prompt.

## Key Insight: Three Calibration Levers

**LLM-as-Judge calibration is prompt engineering, not model selection.** Moving
from GPT-4o-mini to GPT-4o for the judge improved reasoning quality, but the
0% → 20% shift came entirely from prompt changes. The three levers that
mattered most:

1. **Identity**: Tell the model it's strict and that failures are expected.
2. **Specificity**: Replace vague criteria with concrete, checkable conditions.
3. **Anchoring**: Set the expected distribution ("most guides are 3-4", "most
   have 1-2 issues") so the model doesn't default to its positivity bias.

### Before/After Prompts (Condensed)

**v1 (Lenient — 0% failure rate):**

```text
You are a quality evaluator for DIY home repair guides.
Evaluate the following repair guide for these 6 failure modes.
For each mode, respond with 0 (pass) or 1 (fail) and a brief reason.

1. incomplete_answer: Answer lacks sufficient detail
2. safety_violations: Missing or inadequate safety info
3. unrealistic_tools: Tools a homeowner wouldn't have
...
```

**v2 (Calibrated — 20% failure rate)** — from `evaluator.py:_JUDGE_SYSTEM_PROMPT`:

```text
You are a STRICT quality evaluator for DIY home repair guides.
Your job is to find deficiencies. Most guides have at least 1-2 issues.
...
Failure mode definitions (apply these STRICTLY):
1. incomplete_answer: Fail if the answer has fewer than 3 substantive
   sentences, omits key information a homeowner needs...
2. safety_violations: Fail if ANY of these are missing when relevant:
   power-off/water-off warning BEFORE starting work, PPE requirements...
...
Be thorough and critical. A guide that is merely "okay" should still
get failures flagged. Only give 0 (pass) when genuinely strong.
Quality scores: 1=very poor ... 5=excellent. Most guides are 3-4.
```

> Full prompts are shown in the [Decision](#decision) section above.

## Java/TS Parallel

This is analogous to **configuring a linting rule severity**. ESLint's default
`"warn"` on most rules means developers ignore warnings. Changing to `"error"`
with specific rule configurations (e.g., `max-lines-per-function: 50`) forces
actual compliance. The lenient prompt was like running ESLint with all rules set
to `"off"` — technically running but producing no useful output. The strict
prompt is like a carefully tuned `.eslintrc` with rules calibrated to catch real
issues without drowning the team in false positives.
